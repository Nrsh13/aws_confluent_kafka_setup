# Common Errors:

##1. If we use cp-ansible repository without any changes and ansible collections are not configured.

ansible.cfg is the most important as the installation with cp-ansible repo download expects the ansible collection installation etc which means itwill look for a particular location - ~/.ansible/collections/ansible_collections/confluent/platform path and refer this as confluent.platform.xyz in all code. Also, it picks all filter plugins from ~/.ansible/collections/ansible_collections/confluent/platform/plugins/filters folder etc.

All code having confluent.platform.XYZ (#kafka_broker_listeners | confluent.platform.ssl_required(ssl_enabled) or) will throw ERROR saying unable to locate collection confluent.platform.XYZ

for eq.

TASK [zookeeper : Set Zookeeper Data Dir Ownership] **********************************************************************************************
Saturday 15 April 2023  19:10:49 +1200 (0:00:00.083)       0:02:55.585 ********
fatal: [ansi-lab01-01.nrsh13-hadoop.com]: FAILED! => {"msg": "An unhandled exception occurred while templating '{{ zookeeper_combined_properties | combine(zookeeper_custom_properties) }}'. Error was a <class 'ansible.errors.AnsibleError'>, original message: An unhandled exception occurred while templating '{{ zookeeper_properties | confluent.platform.combine_properties }}'. Error was a <class 'ansible.errors.AnsibleError'>, original message: template error while templating string: Could not load \"confluent.platform.combine_properties\": 'Invalid plugin FQCN (confluent.platform.combine_properties): unable to locate collection confluent.platform'. String: {{ zookeeper_properties | confluent.platform.combine_properties }}. Could not load \"confluent.platform.combine_properties\": 'Invalid plugin FQCN (confluent.platform.combine_properties): unable to locate collection confluent.platform'"}

So configuring collection is required -

https://docs.confluent.io/ansible/7.3/ansible-download.html#download-the-cp-collection-from-ansible-galaxy


[root@ansi-lab01-01 cp-ansible]# ansible-galaxy collection install confluent.platform:7.2.5
[DEPRECATION WARNING]: [defaults]callback_whitelist option, normalizing names to new standard, use callbacks_enabled instead. This feature will
be removed from ansible-core in version 2.15. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg.
Starting galaxy collection install process
Process install dependency map
Starting collection install process
Downloading https://galaxy.ansible.com/download/confluent-platform-7.2.5.tar.gz to /root/.ansible/tmp/ansible-local-78874uhine4wj/tmpfczyi70f/confluent-platform-7.2.5-a9vi0_7x
Installing 'confluent.platform:7.2.5' to '/root/.ansible/collections/ansible_collections/confluent/platform'
confluent.platform:7.2.5 was installed successfully




##2. If python is NOT as per the required version for Ansible.

Set python for ansible to use in ansible.cfg
interpreter_python=/usr/bin/python3.9


TASK [common : Upgrade pip] **********************************************************************************************************************
Saturday 15 April 2023  19:01:04 +1200 (0:00:03.954)       0:01:02.390 ********
fatal: [ansi-lab01-01.nrsh13-hadoop.com]: FAILED! => {"changed": false, "cmd": ["/usr/libexec/platform-python", "-m", "pip.__main__", "install", "--upgrade", "pip"], "msg": "stdout: Collecting pip\n  Downloading https://nexus.sharedservices.aws.nz.thenational.com/repository/pypi/packages/pip/23.0.1/pip-23.0.1-py3-none-any.whl (2.1MB)\n\n:stderr: WARNING: Running pip install with root privileges is generally not a good idea. Try `__main__.py install --user` instead.\npip requires Python '>=3.7' but the running Python is 3.6.8\n"}


##3. Authentication/RBAC/MDS setup error -

- Failure at ansible execution -

TASK [kafka_broker : Grant role System Admin to Additional Kafka Broker System Admins] ***********************************************************
Monday 17 April 2023  06:21:11 +1200 (0:00:01.600)       0:04:25.906 **********
FAILED - RETRYING: [ansi-lab01-01.nrsh13-hadoop.com]: Grant role System Admin to Additional Kafka Broker System Admins (30 retries left).
FAILED - RETRYING: [ansi-lab01-02.nrsh13-hadoop.com]: Grant role System Admin to Additional Kafka Broker System Admins (30 retries left).
F

Kafka logs shows below - meaning the mTLS is being used for Authentication while MDS should be with LDAP settings. Make sure kafka_broker_custom_listeners along with LDAP details is configured in the inventory file.  

[2023-04-16 14:43:07,926] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Failed authentication with /10.86.113.95 (channelId=10.86.113.95:9092-10.86.113.95:47914-4900) (errorMessage=SSL handshake failed caused by Empty client certificate chain) (org.apache.kafka.common.network.Selector)
[2023-04-16 14:43:07,974] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Failed authentication with /10.86.113.95 (channelId=10.86.113.95:9092-10.86.113.95:47916-4900) (errorMessage=SSL handshake failed caused by Empty client certificate chain) (org.apache.kafka.common.network.Selector)


##4. If LDAP Details are NOT correct -

TASK [kafka_broker : Wait for Embedded Rest Proxy to start] **************************************************************************************
Monday 17 April 2023  06:52:24 +1200 (0:00:00.233)       0:04:19.480 **********
FAILED - RETRYING: [ansi-lab01-02.nrsh13-hadoop.com]: Wait for Embedded Rest Proxy to start (25 retries left).
FAILED - RETRYING: [ansi-lab01-03.nrsh13-hadoop.com]: Wait for Embedded Rest Proxy to start (25 retries left).
.fatal: [ansi-lab01-02.nrsh13-hadoop.com]: FAILED! => {"attempts": 25, "changed": false, "connection": "close", "content": "{\"error_code\":40101,\"message\":\"Client SASL mechanism 'SCRAM-SHA-512' not enabled in the server, enabled mechanisms are [OAUTHBEARER]\"}", "content_type": "application/json", "date": "Sun, 16 Apr 2023 19:23:09 GMT", "elapsed": 0, "json": {"error_code": 40101, "message": "Client SASL mechanism 'SCRAM-SHA-512' not enabled in the server, enabled mechanisms are [OAUTHBEARER]"}, "msg": "Status code was 401 and not [200]: HTTP Error 401: Unauthorized", "redirected": false, "status": 401, "url": "https://ansi-lab01-02.nrsh13-hadoop.com:8090/kafka/v3/clusters"}


Kafka sever logs - 

[2023-04-17 06:53:14,252] ERROR [AdminClient clientId=adminclient-2] Connection to node -3 (ansi-lab01-03.nrsh13-hadoop.com/10.86.113.148:9091) failed authentication due to: Client SASL mechanism 'SCRAM-SHA-512' not enabled in the server, enabled mechanisms are [OAUTHBEARER] (org.apache.kafka.clients.NetworkClient)
[2023-04-17 06:53:14,252] WARN [AdminClient clientId=adminclient-2] Metadata update failed due to authentication error (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.UnsupportedSaslMechanismException: Client SASL mechanism 'SCRAM-SHA-512' not enabled in the server, enabled mechanisms are [OAUTHBEARER]

##5. When Certifictes used for Producer/Consumer are NOT correct

- when /var/ssl/private/kafka_broker.crt and /var/ssl/private/kafka_broker.key is NOT mentioned properly.
- curl https://ansi-lab01-01.nrsh13-hadoop.com:18081/subjects/mytopic_avro-value/versions/1 --cert /var/ssl/private/kafka_broker.crt --key /var/ssl/private/kafka_broker.key

WARN: Failed to get Schema from SR. Using Default Schema now HTTPSConnectionPool(host='ansi-lab01-01.nrsh13-hadoop.com', port=18081): Max retries exceeded with url: /subjects/mytopic_avro-value/versions (Caused by SSLError(SSLError(1, '[SSL: SSLV3_ALERT_BAD_CERTIFICATE] sslv3 alert bad certificate (_ssl.c:2633)')))

- when root cert is NOT mentioned properly.

WARN: Failed to get Schema from SR. Using Default Schema now HTTPSConnectionPool(host='ansi-lab01-01.nrsh13-hadoop.com', port=18081): Max retries exceeded with url: /subjects/mytopic_avro-value/versions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)')))


##6. When we just have ONE node/broker Installation

Change internal replication Factor below in roles/variables/defaults/main.yml

### Recommended replication factor, defaults to 3. When splitting your cluster across 2 DCs with 4 or more Brokers, this should be increased to 4 to balance topic replicas.
default_internal_replication_factor: 1
	
		
Connect causes broker failure - set below topic.replication to 1 (license topic by default looks for 3 brokers)

[root@ansi-lab01-01 cp-ansible]# 
vi roles/variables/vars/main.yml

kafka_connect_replicator_properties:
  defaults:
    enabled: true
    properties:
      confluent.topic.replication.factor: 1


[2023-04-15 21:50:14,029] ERROR [KafkaServer id=1] Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)
org.apache.kafka.common.KafkaException: Failed to start license store with topic _confluent-command. A valid license must be configured using 'confluent.license' if topic is unavailable.
        at io.confluent.license.validator.ConfluentLicenseValidator.start(ConfluentLicenseValidator.java:144)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:797)
        at kafka.Kafka$.main(Kafka.scala:108)
        at kafka.Kafka.main(Kafka.scala)


##7. If /var/ssl/private folder is NOT used for certs (meaning we are using key/certs from secrets_role), config_validations check should be removed/commented from/in roles/common/tasks/main.yml

#- name: Config Validations
#  include_tasks: config_validations.yml
#  when: validate_hosts|bool
#  tags:
#    - validate

Else Kafka Broker will fail to start with NullPointer Exception.

ERROR [KafkaServer id-2] Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)
java.lang.NullPointerException
  at io.confluent.kafka.server.plugins.auth.token.TokenBearerValidatorCallbackHandler.close(TokenBearerValidatorCallbackHandler.java:101)
  at org.apache.kafka.common.network.SaslChannelBuilder.close(SaslChannelBuilder.java:266)